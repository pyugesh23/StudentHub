API Key prefix: gsk_cTNNAX
Initializing Groq client...

Testing model: llama-3.1-8b-instant
SUCCESS: llama-3.1-8b-instant worked!
Response: Hello. What can I help you with today?

Testing model: llama3-8b-8192
FAILURE for llama3-8b-8192: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
Traceback (most recent call last):
  File "C:\Users\pyuge\OneDrive\Desktop\StudentHub\diag_groq.py", line 30, in main
    response = client.chat.completions.create(
        model=model,
    ...<4 lines>...
        max_tokens=10
    )
  File "C:\Users\pyuge\AppData\Local\Programs\Python\Python313\Lib\site-packages\groq\resources\chat\completions.py", line 461, in create
    return self._post(
           ~~~~~~~~~~^
        "/openai/v1/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<45 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\pyuge\AppData\Local\Programs\Python\Python313\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pyuge\AppData\Local\Programs\Python\Python313\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
